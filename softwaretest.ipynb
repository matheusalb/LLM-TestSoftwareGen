{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import (\n",
    "    load_dataset\n",
    ")\n",
    "import replicate\n",
    "from langchain.llms import Replicate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openai_humaneval (/home/matheusalb/.cache/huggingface/datasets/openai_humaneval/openai_humaneval/1.0.0/2955cebd73602e828fa8c0a424c594e5fab4ec863b316ca98f3d8fdb6a626e75)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4905d7717811490cb40fefa6058d8315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('openai_humaneval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['test'][0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    for idx, elem in enumerate(numbers):\n",
      "        for idx2, elem2 in enumerate(numbers):\n",
      "            if idx != idx2:\n",
      "                distance = abs(elem - elem2)\n",
      "                if distance < threshold:\n",
      "                    return True\n",
      "\n",
      "    return False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['test'][0]['canonical_solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
      "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
      "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
      "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
      "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
      "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['test'][0]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['REPLICATE_API_TOKEN'] = 'r8_SFFfZ4rWUTbfw21S2MZXZVeTRuCkGjb06jw4Y'\n",
    "\n",
    "# meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\n",
    "llm = Replicate(\n",
    "    model=\"meta/codellama-34b-python:482ba325daab209d121f45a0030f2f3ed942df98b185d41635ab3f19165a3547\",\n",
    "    model_kwargs={\"temperature\": 0.01, \"max_tokens\": 1024, \"top_p\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{context}\n",
    "{canonical_solution}\n",
    "\n",
    "def test():\n",
    "'''\n",
    "Test function of the above function. It contains 5 unit test cases fot the above function.\n",
    "'''\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"context\", \"canonical_solution\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executando Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= 13\n",
    "output = chain.run({'context': dataset['test'][index]['prompt'],\n",
    "           'canonical_solution': dataset['test'][index]['canonical_solution']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def greatest_common_divisor(a: int, b: int) -> int:\n",
      "    \"\"\" Return a greatest common divisor of two integers a and b\n",
      "    >>> greatest_common_divisor(3, 5)\n",
      "    1\n",
      "    >>> greatest_common_divisor(25, 15)\n",
      "    5\n",
      "    \"\"\"\n",
      "\n",
      "    while b:\n",
      "        a, b = b, a % b\n",
      "    return a\n",
      "\n",
      "def test():\n",
      "\n",
      "    assert greatest_common_divisor(3, 5) == 1\n",
      "    assert greatest_common_divisor(25, 15) == 5\n",
      "    assert greatest_common_divisor(90, 30) == 30\n",
      "    assert greatest_common_divisor(81, 17) == 1\n",
      "    assert greatest_common_divisor(4, 2) == 2\n",
      "    print(\"Passed unit tests\")\n",
      "    \n",
      "if __name__ == \"__main__\":\n",
      "    test()\n"
     ]
    }
   ],
   "source": [
    "print(dataset['test'][index]['prompt']+ '\\n'+ dataset['test'][index]['canonical_solution'] + '\\n' + 'def test():' + '\\n' + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed unit tests\n"
     ]
    }
   ],
   "source": [
    "exec(dataset['test'][index]['prompt']+ '\\n'+ dataset['test'][index]['canonical_solution'])\n",
    "\n",
    "exec(\"def test():\\n\" + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
